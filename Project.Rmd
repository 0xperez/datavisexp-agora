---
title: "Programming and Visualization for Data Science - Project"
output: html_notebook
author: "Davide Perez Cuevas"
---

```{r setup}
library(tidyverse)
library(lubridate)
library(countrycode)
library(ggplot2)
library(gridExtra)
library(treemapify)
```

## Project task
The aim of
this project is to explore some dataset (possibly a combination of more than one), using the visualization techniques we
study in this course.

For this project you shall carry out at least the following.

* Select one or more data sources. You should find one or more datasets that are interesting for you. You are free to pick any dataset you prefer, on OLE there is a list of pointers to useful data repositories.

* Describe what the datasets are about and what you expect to find during the exploration.

* Clean and preprocess the dataset, recording the reasoning behind your preprocessing choices in the report.

* Visualize different aspects of the dataset using the most appropriate visualizations we study in the course.

* State your findings.


## Project description
"Agora" was one of the largest darknet markets, and was active from 2013 to 2015. It operated primarily as a black market, allowing transactions of illegal items and services such as drugs, weapons, stolen/fake documents, cyber arms...
The dataset under analysis is a Kaggle dataset (find it [here](https://www.kaggle.com/philipjames11/dark-net-marketplace-drug-data-agora-20142015)) created from a raw html rip of the Agora website (years 2014/2015), which was disclosed by an unknown Reddit user and which lead, probably, to the Agora shutdown a few months after. The Kaggle dataset curator states that he obtained the data from a 3rd party source, but acknowledges its origin from the [Darknet Web Archives](https://www.gwern.net/DNM-archives), a huge collection of scraped/mirrored data from the Dark Net Markets between years 2013-2015 and used as a reference from many papers related to studies on darknet markets. Moreover, he advises that some operations already have been made to the dataset: duplicated listings have been merged into a single listing, and the price has been averaged across merged duplicates.

The exploration of this dataset could lead to some interesting findings. Examples are potential correlations between different illegal items and from their shipping origin/destination, price estimate in certain regions in the world and identifying high risk regions or vendors.

Bitcoin is said to be the de facto currency of the dark web, and was used as currency in this market too. A time-series dataset, which we extracted from the archives of a [stock exchange website](https://www.investing.com/), is used as a support to achieve a precise conversion of the original bitcoin price to the euro currency.
**DISCLAIMER: darkweb is a treacherous dimension, and most of transaction on darknet markets involve illegal items. The following does not intend neither to encourage illegal activities in any way, nor to promote reckless darkweb surfing. Rather, it aims to perform an analysis to try to identify the past behaviours of one of the biggest of such market platforms.**

## Dataset description
*DISCLAIMER n.2: in the follwing, we will use the term dataset as a broad term to refer both the data collection in analysis and the parsed data in R in form of a dataframe or tibble object.*

As first step, we read the dataset from the corresponding .csv file.
```{r message = FALSE}
agora_raw <- read_csv("data/Agora.csv")
```
Then we get a first glimpse (in a very real sense) of the information in it:
```{r}
glimpse(agora_raw)
```
We see that the Agora dataset contains more than 100000 entries about different products and 9 features.
Here we give a description of the features as in Kaggle:

* `Vendor`: username of the seller

* `Category`: Agora's marketplace category where the item belongs

* `Item`: item or service name

* `Description`: a description of the listing

* `Price`: cost of the item in bitcoins (BTC). The price has been averaged across any duplicated listings, and duplicates have been discarded.

* `Origin`: country name or code of the country the item is listed to have shipped from

* `Destination`: country name or code wheee the item is listed to be shipped to

* `Rating`: the rating given to the seller on the Agora marketplace, on a 0-5 scale

* `Remarks`: contains notes about the listing

## Analysis overview
Web scrapes may be difficult to analyze: they often are large, redundant and highly error-prone. This is particularly true for scrapes coming from black webmarkets: being located in the darkweb and treating illegal matters, they are highly unstable. Products and vendors may be banned and disappear from day to day, metadata such as category descriptions or shipping information can be wrong or intentionally misleading, prices fluctuation is high due to cryptocurrencies use. Data is not well organized because often the website design is kept minimal and input is not sanitized or constrained due to anonimity concerns. Thus, such data will never be comprehensive and give an overall, long-term view on the market situation, but it rather represents a (more or less complete) snapshot of the market in a given moment. Our dataset is not an exception, and even by giving a quick `glimpse` we understand it is quite dirty.

Despite the intrinsic incompleteness, a complete enough dataset such as this one can give very helpful insights, but it requires quite a lot of cleaning and processing to be as reliable as possible.
Given this, the analysis we will carry on will not follow a *waterfall* flow, in the sense that it will not be a linear process chaining cleaning, preprocessing and exploration tasks in a linear way. Rather, it will be an *iterative process* in which the analysis phases are intertwined, depending on the aspect we are analyzing.
The following example will help to better understand this approach. Suppose we want to do a preliminary dataset cleaning involving all features, prior beginning any exploratory task. Suppose we begin by inspecting the `Destination` column.
```{r}
agora_raw %>% 
  distinct(Destination)
```
We immediately notice some big problems. No coherent naming system was used to represent countries. Sometimes the feature is a country code, sometimes it is an alias, sometimes the full country name, sometimes it is a list of countries or simply something that does not represent a country at all.
The data is too scrambled to fix all entries manually or via string manipulation, but wiping out all the listings with an invalid information would cost us a big chunk of our data and the loss of precious information. Even worse, the `Origin` feature suffers from the same problems, and this will further reduce the number of valid listings.
On the other hand, not all kind of insights we are going to make involve this feature: for example, if we want to know the top vendors of the market we are not interested in knowing shipping-related information, and if we purged listings because of invalid information for such features our analysis will inevitably become corrupted.
Some things have to be handled as soon as possible, such as misparsed rows that could taint any type of exploration, but we will postpone the in-depth processing of single features until they are required for an analysis.

## Handling missing and invalid values
We inspect the missing values per column:

```{r}
#TODO: substitute deprecated funs with a lambda or a function list
agora_raw %>%
  select(everything()) %>%
  summarise_all(funs(sum(is.na(.))))
```

We see that for almost all features there is a number of entries with missing values.
We compute the percentage of null values to get an idea of the portion of data affected:

```{r}
#TODO: substitute deprecated funs with a lambda or a function list
 agora_raw %>% 
   summarise_each(funs( 100 * round(mean( is.na(.) ), 6 )))
 #agora_raw %>% 
   #summarize(
     #across(agora_raw, ( 100 * round(mean( is.na(.) ), 6 )))
     #)
```

We have now a clearer idea: for `Remarks` column, almost all data is missing, while for `Destination` column a solid 44,81% of the listings have a missing value.
For what concerns `Destination`, the Kaggle dataset description states that an empty value probably represents an item being shipped worldwide: by means of a quick research on the website the dataset was originally published on and on the archives it references, like [this one](https://archive.org/details/dnmarchives), seems that a missing `Origin` or `Destination` means that the item is shipped from or ships to worldwide, indeed. So we can assume that missing values in such columns have such meaning.
However, the `Remarks` feature is not so relevant, so we can safely drop it.
```{r}
agora <-
  agora_raw %>%
  select(-c('Remarks'))
```

The `Item` feature is an information that is required for a valid listing in any marketplace: a missing value could indicated a misparsed entry. Same reasoning goes for the `Price` feature.
```{r}
agora %>%
  filter(is.na(`Item`) | is.na(`Price`))
```
Indeed, these rows are either scrambled or incomplete. We proceed removing all of them.
```{r}
agora <- 
  agora %>%
  filter(!is.na(Item) & !is.na(Price))
```
*Item Description* contains text giving additional info about the related product, so it is not object of analysis and can be fully dropped.
```{r}
agora <- 
  agora %>%
  select(-c('Item Description'))
```


We managed to do a preliminary shrinking of the dataset by getting rid of redundant information.

## Exploration

### Agora's Top 20
We want to know who are the most active vendors in terms of listings.

```{r}
agora %>% 
  distinct(Vendor) %>% 
  count()
```
On the marketplace there are almost 3200 vendors. We focus our attention on the top 20 ones.

```{r message=false}
top_20_vendors <- 
  agora %>%
  group_by(Vendor) %>%
  summarise(N_items = n()) %>%
  arrange(desc(N_items)) %>%
  slice_head(n = 20)

top_20_vendors %>% 
    ggplot(aes(x = N_items, y = reorder(Vendor, N_items))) +
    geom_col() +
    labs(title = 'Top 20 vendors per no. of items', x = 'No. of items on sale', y = 'Vendor')
```
We see that the top four vendors have a similar number of items listed, while from the fifth on there is a noticeable gap.

We visualize the same data by means of the so-called treemap. The bigger the rectangle, the bigger the number of items on sale from the related vendor.
While this visualization is less effectve than the bar chart in terms of quantifying sold items, it highlights better other insights about this data. For example, it is evident that the top four vendors (stacked at the left handside of the plot) have a comparable sales volume.
```{r}
top_20_vendors %>%
  ggplot(aes(area = N_items, fill = Vendor, label = Vendor)) +
  geom_treemap(show.legend = FALSE) +
  geom_treemap_text()
```
Another indicator of a vendor's weight on the market could be the *Rating* feature. It expresses the average rating given to the seller from other users on the marketplace.
```{r}
agora %>%
  select(Vendor, Rating) %>%
  distinct() %>%
  arrange(desc(Rating))
```
First thing we notice are two misparsed row, which we remove. 
```{r}
agora <- 
  agora %>%
  filter(Rating != 'Worldwide') %>%
  filter(Rating != 'USA')
```
Secondly, we must clean the feature in some way.
```{r}
agora %>%
  select(Rating) %>%
  distinct()
```
We start handling missing/literal values: they mean that the related vendor has not closed enough deals yet to compute a meaningful rating.
In this case, it makes sense to use NA as an indicator that the rating is not present at all.
```{r}
agora <-
  agora %>%
  mutate(Rating = ifelse(str_detect(Rating, "deals"), NA, Rating))
```
Then we clean the other invalid values.
```{r}
agora <-
  agora %>%
  mutate(Rating = str_remove_all(Rating, "~")) %>%
  mutate(Rating = str_remove_all(Rating, "/5")) %>%
  mutate(Rating = as.numeric(Rating))
```
We successfully processed the Rating feature.
```{r}
agora %>%
  select(Rating) %>%
  distinct() %>%
  arrange(desc(Rating))
```
```{r}
agora %>%
  select(Vendor, Rating) %>%
  distinct() %>%
  ggplot(aes(x = Rating)) +
  geom_histogram()
```
We see that bad ratings are very rare: most ratings are among 4 and 5, with a very high number of vendors with top ratings. **This may mean that the market is a "safe" place where to buy the kind of items it offers**.




We now focus our attention on the different categories of the market.
```{r}
agora %>% 
  distinct(Category) %>% 
  count()
```
There are 105 different categories, which is quite a huge number for classifying items.
```{r}
agora %>% 
  distinct(Category) %>%
  arrange(Category)
```
We immediately notice a couple misparsed entry, where the price value is in the category instead: we proceed to selectively remove it.
```{r}
agora <- 
  agora %>%
  filter(Category != '0.12780125125 BTC') %>%
  filter(Category != '0.1905617980645162 BTC')
```
Now we do a quick inspection of the number of items per category.
```{r}
categories <-
  agora %>%
  group_by(Category) %>%
  summarise(N_items = n()) %>%
  arrange(desc(N_items))
categories
```
From this preliminary check, we can already say that **drug-related products seems to have a great weight in the market.**.

```{r}
arrange(categories, N_items)
```
By inspecting the less popular categories, we found some other misparsed entries which we remove from the original dataset.
```{r}
agora <-
  agora %>%
  filter(!str_detect(Category, "Body Bags"))
```

However, there still is a problem. It looks like each category is divided in one or more subcategories (or tiers). For example, the category *Drugs/Cannabis/Weed* identifies a main category *Drugs* and two further subcategories *Cannabis* and *Weed*. We may want to do some more fine-grained exploration of the categories, for example exploring one tier at time.
We check how many tiers there are.
```{r}
categories <-
  categories %>%
  mutate(Depth = str_count(Category, "/") + 1) %>%
  arrange(desc(Depth))
categories
```
We see there are up to four category tiers.
We will analyze the different categories in detail: obviously, it makes sense to explore them depending on the tier level in order to keep the analysis tidy.
```{r}
categories <-
  agora %>%
  select(Category) %>%
  mutate(Depth = str_count(Category, "/") + 1) %>%
  separate(Category, into = c("Tier1", "Tier2", "Tier3", "Tier4"), sep = "/")
categories
```
To speed things up, we build a couple functions to extract a tidy tier-specific dataset and to plot it in various formats.
```{r}
get_tier_data <- function(dataset, tier_no, results_no){
  col_name = sprintf("Tier%d", tier_no)
  tier_data <-
    dataset %>%
    select(col_name) %>%
    drop_na(col_name) %>%
    group_by_at(col_name) %>%
    summarise(N_items = n()) %>%
    arrange(desc(N_items)) %>%
    slice_head(n = results_no)
  return(tier_data)
}

plot_tier_bar <- function(dataset, tier_no, results_no){
  col_name = sprintf("Tier%d", tier_no)
  p <-
    ggplot(get_tier_data(dataset, tier_no, results_no), aes(x = N_items, y = reorder(!!sym(col_name), N_items))) + # col_name is a string, must be a symbol so we use r features to convert it
    geom_col() +
    scale_fill_brewer(palette = "Dark2") +
    labs(title = sprintf("Top %d entries of cat. tier %d", results_no, tier_no), x = "No. of items", y = "Category")
  return(p)
}

plot_tier_pie <- function(dataset, tier_no, results_no){
  col_name = sprintf("Tier%d", tier_no)
  pie_tier <-
    ggplot(get_tier_data(dataset, tier_no, results_no), aes(x = "", y = (N_items / sum(N_items) * 100), fill = !!sym(col_name))) +
    geom_bar(stat = "identity", width = 1, color = "white") +
    coord_polar("y", start = 0) +
    theme_void() + 
    labs(title = sprintf("Market shares for top %d items", results_no, tier_no), y = "Test") +
    geom_text(aes(label = round((N_items / sum(N_items) * 100), digits = 2)), position = position_stack(vjust = 0.5)) +
    scale_fill_brewer(palette = "Set1")
  return(pie_tier)
}
```

We get a global overview by plotting all of the tiers in a grid. We concentrate on the top 15 entries.
```{r}
p1 <- plot_tier_bar(categories, 1, 15)
p2 <- plot_tier_bar(categories, 2, 15) 
p3 <- plot_tier_bar(categories, 3, 15)
p4 <- plot_tier_bar(categories, 4, 15) 
grid.arrange(p1, p2, p3, p4, ncol = 2)
```
By means of these simple grid of plots we see some important characteristics of the market: the drugs dominates all the other product types, and among them cannabis and weed are by far the most sold products. 

We can have a better visualization of this by means of pie charts showing the market shares for each category tier.
```{r}
pie1 <- plot_tier_pie(categories, 1, 5)
pie2 <- plot_tier_pie(categories, 2, 5)
pie3 <- plot_tier_pie(categories, 3, 5)
pie4 <- plot_tier_pie(categories, 4, 5)
grid.arrange(pie1, pie2, pie3, pie4, ncol = 2)
```
Again, considering the first five categories, we see that the drug market has a market share of over 90%. In tier 2 and tier 3, indeed, we only find subcategories of drugs in the top five. 
For tier 2 cannabis has the top market share, taking almost the half of the market share, followed by ecstasy and stimulants.
For tier 3, weed is the top product and greatly surpasses the second and third one (pills and cocaine), which have a similar market share.

Market share of tier 4 is not so relevant because there are very few items having this classification compared to the total number of listings:
```{r}
categories %>%
  filter(!is.na(Tier4)) %>%
  count()
```
We may hypotize this happened so due to a wrong classification, either by the user selling the product or by the website itself. However, for our analysis' sake, we may eliminate this less-relevant tier 4 and reassign those items to the upper-level tier. Note that we are not elimining any entry, we are *reclassifying* it.
```{r}
agora <-
  agora %>%
  mutate(Category, Category = ifelse(str_detect(Category, "Sex"),"Info/eBooks/Relationships", Category)) %>%
  mutate(Category, Category = ifelse(str_detect(Category, "trim"),"Drugs/Cannabis/Shake", Category)) %>%
  mutate(Category, Category = ifelse(str_detect(Category, "UFOs"),"Info/eBooks/Aliens", Category))
```
Once we drawn a preliminary profile of the market categories, we now focus on prices.
As in every darkweb market, prices are in expressed in bitcoins. Moreover, the feature is a string which encodes both the price and the currency. We need to make it tidy by converting it and getting tid of the currency, which is always the same for all prices.
```{r}
agora <-
  agora %>%
  separate(Price, into = c("PriceBTC", "Currency"), sep = " ") %>%
  mutate(PriceBTC = as.numeric(PriceBTC)) %>%
  select(-c("Currency"))
agora
```
We validate the new feature by checking for null values.
```{r}
agora %>%
  filter(is.na(PriceBTC))
```
We found some other misparsed entries. Luckily they are a few, so we drop them.
```{r}
agora <-
  agora %>%
  filter(!is.na(PriceBTC))
```
Despite the feature being now tidy, it still is not so informative. Prices expressed in Bitcoin are less informative than prices expressed in standard currencies. This mainly for two reasons:

* Agora dataset is from years 2014/2015. Bitcoin value back in those years is probably very different from its current one.
* People are used to express values with standard currencies. Knowing that a price is 0.025 BTC give a less precise idea than knowing it is 3882 euro, even for those who does not use it as their main currency.

So we need to convert the original price to another currency. The problem is that Bitcoin is very volatile: its value quickly changes on a daily basis, and *we are dealing with data covering a timespan of two years and without an indication about the exact date at which a particular item was on sale*. Thus, we need a precise method to achieve a meaningful enough conversion factor to craft a new price feature.
For this task, we use a dataset which we extracted from [Investing.com](https://www.investing.com/), a reputable website offering stock market services. Since Bitcoin/Euro exchange rate is listed on the stock exchange, we can easily find data related to its value over a certain period.
```{r message = FALSE}
btc_eur <- read_csv("data/BTCHist01012014.csv")
```
```{r}
btc_eur
```
This dataset is a snapshot of the financial market related to BTC/EUR exchange rate over the time period 01/01/2014 - 30/11/2020.
A market analysis on the fluctuation of Bitcoin value is complex and out of the scope of this project, but to have an idea of the volatility of this cryptocurrency we can see the first and last entry of this dataset:
```{r}
head(btc_eur, n = 1)
tail(btc_eur, n = 1)
```
We see that on date 01/01/2014 the BTC/EUR exchange rate was `815.9`. Almost six years later, on 30/11/2020, the same rate is `18646.0`.
But we don't need to look over a very wide timespan to understand Bitcoin's volatility. Let's compare years of the period which matters to us, namely years 2014-2015:
```{r}
btc_eur %>%
  filter(Date == "Jan 01, 2014") %>%
  head(n = 1)
```
In date 01/01/2014, one bitcoin was worth `815.9` euros.
```{r}
btc_eur %>%
  filter(Date == "Jan 01, 2015") %>%
  head(n = 1)
```
Exactly one year later, its value is less than a half!
This clearly may represent a problem in our analysis, because we lack a time reference indicating the exact item date of sale. So we use this dataset as a mean to obtain an exchange rate which is as reliable as possible.

First we clean and filter the dataset to suit our needs. We are only interested to the date and the price in a specific timespan: from a [quick search](https://en.wikipedia.org/wiki/Agora_%28online_marketplace%29) we see that the website shut down in August 2015, so we can narrow the interval.
```{r}
btc_eur <-
  btc_eur %>%
  select(`Date`, Price) %>%
  mutate(`Date` = mdy(`Date`)) %>%
  filter(`Date` < as_date("2015-08-31"))
btc_eur
```
```{r}
btc_eur %>%
  ggplot(aes(x = Date, y = Price)) +
  geom_line(na.rm = TRUE) +
  labs(x = "Date", y = "BTC/EUR") +
  scale_y_continuous(labels = scales::dollar_format(suffix = "€", prefix = ""))
```
As we expected, exchange rate was quite variable in these years too. We compute the average value and take it as factor to convert bitcoins to euro:
```{r}
btc_eur_rate <-
  mean(btc_eur[["Price"]])
btc_eur_rate
```
We can now create a new column in our dataset.
```{r}
agora <-
  agora %>%
  mutate(PriceEUR = round(PriceBTC * btc_eur_rate, 2), .after = PriceBTC) %>%
  mutate(PriceBTC = round(PriceBTC, 4))
agora
```



Now that we have reliable pricing info, we can go back on the categories information obtain new insights (remember that cat. tier 4 has been merged with tier 3).
```{r}
categories <-
  agora %>%
  mutate(Depth = str_count(Category, "/") + 1) %>%
  mutate(Depth = as_factor(Depth)) %>%
  separate(Category, into = c("Tier1", "Tier2", "Tier3"), sep = "/")
```
We use a boxplot to visualize the prices per category tier.
```{r}
categories %>%
  ggplot(aes(x = Depth, y = PriceEUR)) + 
  geom_boxplot(outlier.colour = "red", outlier.shape = 1, outlier.size = 4) +
  scale_y_log10(labels = scales::dollar_format(suffix = "€", prefix = ""), 
                breaks = scales::trans_breaks("log10", function(x) 10^x)) +
  labs(title = "Prices per Category Tier", x = "Category Tier", y = "Price")
```
We see a noticeable number of red circles. However, this does not automatically classify them as outliers: identification of such values is tricky and should not be done only relying on boxplots only. This is even more true when dealing with market prices, where many market-related parameters should be taken into account. More probably, these outliers are just values that do not fall in a normal distribution: in other words, there is a number of items whose price is very high and far away from the median price and the 75th percentile.

Zooming on the categories, we notice the same behavior:
```{r}
categories %>%
  select(Tier1, PriceEUR) %>%
  drop_na() %>%
  ggplot(aes(x = Tier1, y = PriceEUR)) + 
  geom_boxplot(outlier.colour = "red", outlier.shape = 1, outlier.size = 4) +
  scale_y_log10(labels = scales::dollar_format(suffix = "€", prefix = ""),
                breaks = scales::trans_breaks("log10", function(x) 10^x)) +
  scale_x_discrete(guide = guide_axis(n.dodge=3)) +
  labs(title = "Detailed Prices for Category Tier 1", x = "Category", y = "Price")

categories %>%
  select(Tier2, PriceEUR) %>%
  drop_na() %>%
  ggplot(aes(Tier2, PriceEUR)) + 
  geom_boxplot(outlier.colour = "red", outlier.shape = 1, outlier.size = 4) +
  scale_y_log10(labels = scales::dollar_format(suffix = "€", prefix = ""),
                breaks = scales::trans_breaks("log10", function(x) 10^x)) +
  scale_x_discrete(guide = guide_axis(n.dodge = 7)) +
  labs(title = "Detailed Prices for Category Tier 2", x = "Category", y = "Price")

categories %>%
  select(Tier3, PriceEUR) %>%
  drop_na() %>%
  ggplot(aes(Tier3, PriceEUR)) + 
  geom_boxplot(outlier.colour = "red", outlier.shape = 1, outlier.size = 4) +
  scale_y_log10(labels = scales::dollar_format(suffix = "€", prefix = ""),
                breaks = scales::trans_breaks("log10", function(x) 10^x)) +
  scale_x_discrete(guide = guide_axis(n.dodge = 7)) +
  labs(title = "Detailed Prices for Category Tier 3", x = "Category", y = "Price")
```
However, these boxplot give a nice visual feedback about the price distribution on different levels. For example, in each of them we can see that prices for some categories, such as Drugs, fluctuates a lot.

#TODO: geographical analyisis (clean values before)





