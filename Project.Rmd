---
title: "Programming and Visualization for Data Science - Project"
output: html_notebook
author: "Davide Perez Cuevas"
---

```{r setup}
library(tidyverse)
library(lubridate)
library(ggplot2)
library(gridExtra)
library(treemapify)
```

## Project task
The aim of
this project is to explore some dataset (possibly a combination of more than one), using the visualization techniques we
study in this course.

For this project you shall carry out at least the following.

* Select one or more data sources. You should find one or more datasets that are interesting for you. You are free to pick any dataset you prefer, on OLE there is a list of pointers to useful data repositories.

* Describe what the datasets are about and what you expect to find during the exploration.

* Clean and preprocess the dataset, recording the reasoning behind your preprocessing choices in the report.

* Visualize different aspects of the dataset using the most appropriate visualizations we study in the course.

* State your findings.


## Project description
"Agora" was known as one of the largest darknet markets. Active from 2013 to 2015, it operated primarily as a black market, allowing transactions of illegal items and services such as drugs, weapons, stolen/fake documents and other goods.
The dataset under analysis is a Kaggle dataset (find it [here](https://www.kaggle.com/philipjames11/dark-net-marketplace-drug-data-agora-20142015)) created from a raw html rip of the Agora website (years 2014/2015), which was disclosed by an unknown Reddit user and which lead, probably, to the Agora shutdown a few months after. The Kaggle dataset curator states that he obtained the data from a 3rd party source, but acknowledges its origin from the [Darknet Web Archives](https://www.gwern.net/DNM-archives), a huge collection of scraped/mirrored data from the Dark Net Markets between years 2013-2015 and used as a reference from many papers related to studies on darknet markets. Moreover, he advises that some operations already have been made to the dataset: duplicated listings have been merged into a single listing, and the price has been averaged across merged duplicates.

Bitcoin is said to be the de facto currency of the dark web, and was used as currency in this market too. A time-series dataset, which we extracted from the archives of a [stock exchange website](https://www.investing.com/), is used as a support to achieve a precise conversion of the original bitcoin price to the euro currency.

The exploration of this dataset could lead to some interesting findings: besides the market analysis, which could translate to classifying items, their price trends and the related vendors, the shipping information allows to investigate the declared origin and destination of the illegal goods, identifying the high risk regions and routes.

**DISCLAIMER: darkweb is a treacherous dimension, and most of transaction on darknet markets involve illegal goods and activities. The following does not intend neither to encourage illegal activities in any way, nor to promote reckless darkweb surfing. Rather, it aims to perform an analysis to try to identify the insights of one of the biggest of such market platforms.**

## Dataset description
**DISCLAIMER n.2: in the follwing, we will use "dataset" as a broad term to refer both the data in analysis and the parsed data in R in form of a dataframe or tibble object.**

As first step, we read the dataset from the corresponding .csv file and get a first glimpse of the information in it.
```{r message = FALSE}
agora_raw <- read_csv("data/Agora.csv")
glimpse(agora_raw)
```
We see that the Agora dataset contains more than 100000 entries and 9 columns.
Here we give a description of the features as declared in Kaggle:

* `Vendor`: username of the seller

* `Category`: Agora's marketplace category where the item belongs

* `Item`: item or service name

* `Description`: a description of the listing

* `Price`: cost of the item in bitcoins (BTC). The price has been averaged across any duplicated listings, and duplicates have been discarded.

* `Origin`: name or code of the country the item is declared to ship from

* `Destination`: name or code where the item is declared to ship to

* `Rating`: rating given to the seller on the Agora marketplace, on a 0-5 scale. Requires the seller to have close a minimum number of transactions.

* `Remarks`: additional notes about the listing


## Analysis overview
Web scrapes may be difficult to analyze: they often are large, redundant and highly error-prone. This is particularly true for scrapes coming from black webmarkets: being located in the darkweb and treating illegal matters, they are highly unstable. Products and vendors may be banned and disappear from day to day, metadata such as category descriptions or shipping information can be wrong or intentionally misleading, prices fluctuation is high due to cryptocurrencies. Data is often both not well organized and not sanitized due to anonimity concerns. Thus, such data will never be comprehensive and give an overall, long-term view on the market situation, but it rather represents a (more or less complete) snapshot of the market in a given moment. Our dataset is not an exception, and even by giving a quick `glimpse` we understand it is quite dirty.

Despite the intrinsic incompleteness, a large enough dataset such as this one can give very helpful insights, but it requires quite a lot of cleaning and processing to be as reliable as possible.
Given this, the analysis we will carry on will not follow a *waterfall* flow, in the sense that it will not be a procedure chaining cleaning, preprocessing and exploration tasks in a linear way. Rather, it will be an *iterative process* in which the analysis phases are intertwined, depending on the aspect we are analyzing. This is done in order to retain as much data as possible. For example, if we want to know the top vendors of the market we are not necessarily interested in knowing shipping-related information, and if we purged listings because of invalid information for such columns we would lose data and our analysis would become unreliable.
Some things have to be handled as soon as possible, such as misparsed rows that could taint any type of exploration, but we will postpone the in-depth processing of single features until they are required.


## Handling missing and invalid values
We inspect the missing values per column by computing the percentage of null values per column:

```{r}
agora_raw %>% 
  summarise_each(funs(100 * round(mean(is.na(.)), 6)))
```

We see that some columns contains a lot of missing values: for `Remarks` column almost all data is missing, while for `Destination` column a solid 44,81% of the listings have a missing value.
The `Remarks` feature is not relevant, so we can safely drop it.
```{r}
agora <-
  agora_raw %>%
  select(-c('Remarks'))
```
The `Item` feature is an information that is required for a valid listing in any marketplace: a missing value probably indicates a misparsed entry. Same reasoning goes for the `Price` feature.
```{r}
agora %>%
  filter(is.na(`Item`) | is.na(`Price`))
```
Indeed, these rows are either scrambled or incomplete. We proceed removing all of them.
```{r}
agora <- 
  agora %>%
  filter(!is.na(Item) & !is.na(Price))
```
*Item Description* contains text giving additional info, so it is not object of analysis and can be fully dropped.
```{r}
agora <- 
  agora %>%
  select(-c('Item Description'))
```
We managed to do a preliminary shrinking of the dataset by getting rid of redundant information.

## Exploration

### Agora's Top Vendors
We want to know who are the most active vendors in terms of listings.
```{r}
agora %>% 
  distinct(Vendor) %>% 
  count()
```
On the marketplace there are almost 3200 vendors. We focus our attention on the top 20 ones.
```{r message=false}
agora %>%
  group_by(Vendor) %>%
  summarise(N_items = n()) %>%
  arrange(desc(N_items)) %>%
  slice_head(n = 20) %>% 
  ggplot(aes(x = N_items, y = reorder(Vendor, N_items))) +
  geom_col() +
  labs(title = 'Top 20 vendors per no. of items', x = 'No. of items on sale', y = 'Vendor')
```
We see that the top four vendors have a similar number of items listed, while from the fifth on there is a noticeable gap.

We visualize the same data by means of the so-called treemap. The bigger the rectangle, the bigger the number of items on sale from the related vendor.
While this visualization is less effectve than the bar chart in terms of quantifying sold items, it highlights better other insights about this data. For example, it is evident that the top four vendors (stacked at the left handside of the plot) have a comparable sales volume.
```{r}
top_20_vendors %>%
  ggplot(aes(area = N_items, fill = Vendor, label = Vendor)) +
  geom_treemap(show.legend = FALSE) +
  geom_treemap_text()
```
Another indicator of a vendor's weight on the market could be the *Rating* feature. It expresses the average rating given to the seller from other users on the marketplace.
```{r}
agora %>%
  select(Vendor, Rating) %>%
  distinct() %>%
  arrange(desc(Rating))
```
First thing we notice are two misparsed rows, which we remove. 
```{r}
agora <- 
  agora %>%
  filter(Rating != 'Worldwide') %>%
  filter(Rating != 'USA')
```
Secondly, we must clean the feature in some way.
```{r}
agora %>%
  select(Rating) %>%
  distinct()
```
We start handling missing/literal values: they mean that the related vendor has not closed enough deals yet to compute a meaningful rating.
In this case, it makes sense to use NA as an indicator that the rating is not present at all.
```{r}
agora <-
  agora %>%
  mutate(Rating = ifelse(str_detect(Rating, "deals"), NA, Rating))
```
Then we clean the other invalid values.
```{r}
agora <-
  agora %>%
  mutate(Rating = str_remove_all(Rating, "~")) %>%
  mutate(Rating = str_remove_all(Rating, "/5")) %>%
  mutate(Rating = as.numeric(Rating))
```
We successfully processed the Rating feature.
```{r}
agora %>%
  select(Rating) %>%
  distinct() %>%
  arrange(desc(Rating))
```
We now visualize it.
```{r}
agora %>%
  select(Vendor, Rating) %>%
  distinct() %>%
  ggplot(aes(x = Rating)) +
  geom_histogram()
```
We see that bad ratings are very rare: most ratings are among 4 and 5, with a very high number of vendors with top ratings.

###Item Categories and Pricing
We now focus our attention on the different categories of the market.
```{r}
agora %>% 
  distinct(Category) %>% 
  count()
```
There are 104 different categories, which is quite a large number for classifying items.
```{r}
agora %>% 
  distinct(Category) %>%
  arrange(Category)
```
We immediately notice a couple misparsed entry: we proceed to selectively remove it.
```{r}
agora <- 
  agora %>%
  filter(Category != '0.12780125125 BTC') %>%
  filter(Category != '0.1905617980645162 BTC')
```
Now we do a quick inspection of the number of items per category.
```{r}
categories <-
  agora %>%
  group_by(Category) %>%
  summarise(N_items = n()) %>%
  arrange(desc(N_items))
categories
```
From this preliminary check, we can already see that drug-related products seems to have a great weight in the market.
```{r}
arrange(categories, N_items)
```
By inspecting the less popular categories, we found some other misparsed entries which we remove from the original dataset.
```{r}
agora <-
  agora %>%
  filter(!str_detect(Category, "Body Bags"))
```
However, there still is a problem. It looks like each category is divided in one or more subcategories (or tiers). For example, the category *Drugs/Cannabis/Weed* identifies a main category *Drugs* and two subcategories *Cannabis* and *Weed*. We may want to do some more fine-grained exploration of the categories, for example exploring one tier at time.
We check how many tiers there are.
```{r}
categories <-
  categories %>%
  mutate(Depth = str_count(Category, "/") + 1) %>%
  arrange(desc(Depth))
categories
```
We see there are up to four category tiers.
We will analyze the different categories in detail: it makes sense to explore them depending on the tier level in order to keep the analysis tidy.
```{r}
categories <-
  agora %>%
  select(Category) %>%
  mutate(Depth = str_count(Category, "/") + 1) %>%
  separate(Category, into = c("Tier1", "Tier2", "Tier3", "Tier4"), sep = "/")
categories
```
To speed things up, we build a couple functions to extract a tidy tier-specific dataset and to plot it in various formats.
```{r}
get_tier_data <- function(dataset, tier_no, results_no){
  col_name = sprintf("Tier%d", tier_no)
  tier_data <-
    dataset %>%
    select(col_name) %>%
    drop_na(col_name) %>%
    group_by_at(col_name) %>%
    summarise(N_items = n()) %>%
    arrange(desc(N_items)) %>%
    slice_head(n = results_no)
  return(tier_data)
}

plot_tier_bar <- function(dataset, tier_no, results_no){
  col_name = sprintf("Tier%d", tier_no)
  p <-
    ggplot(get_tier_data(dataset, tier_no, results_no), aes(x = N_items, y = reorder(!!sym(col_name), N_items))) + # col_name is a string, must be a symbol so we use r features to convert it
    geom_col() +
    scale_fill_brewer(palette = "Dark2") +
    labs(title = sprintf("Top %d entries of cat. tier %d", results_no, tier_no), x = "No. of items", y = "Category")
  return(p)
}

plot_tier_pie <- function(dataset, tier_no, results_no){
  col_name = sprintf("Tier%d", tier_no)
  pie_tier <-
    ggplot(get_tier_data(dataset, tier_no, results_no), aes(x = "", y = (N_items / sum(N_items) * 100), fill = !!sym(col_name))) +
    geom_bar(stat = "identity", width = 1, color = "white") +
    coord_polar("y", start = 0) +
    theme_void() + 
    labs(title = sprintf("Market shares for top %d items", results_no, tier_no), y = "Test") +
    geom_text(aes(label = round((N_items / sum(N_items) * 100), digits = 2)), position = position_stack(vjust = 0.5)) +
    scale_fill_brewer(palette = "Set1")
  return(pie_tier)
}
```
We get a global overview by plotting all of the tiers in a grid. We concentrate on the top 15 entries.
```{r}
p1 <- plot_tier_bar(categories, 1, 15)
p2 <- plot_tier_bar(categories, 2, 15) 
p3 <- plot_tier_bar(categories, 3, 15)
p4 <- plot_tier_bar(categories, 4, 15) 
grid.arrange(p1, p2, p3, p4, ncol = 2)
```
By means of these simple grid of plots we see some important characteristics of the market: the drugs dominates all the other product types, and in this category cannabis and weed are by far the most sold products. 
We can have a better visualization of this by means of pie charts showing the market shares for each category tier.
```{r}
pie1 <- plot_tier_pie(categories, 1, 5)
pie2 <- plot_tier_pie(categories, 2, 5)
pie3 <- plot_tier_pie(categories, 3, 5)
pie4 <- plot_tier_pie(categories, 4, 5)
grid.arrange(pie1, pie2, pie3, pie4, ncol = 2)
```
Again, considering the first five categories, we see that the drug market has a market share of over 90%. In tier 2 and tier 3, indeed, we only find subcategories of drugs in the top five. 
For tier 2 cannabis has the top market share, taking almost the half of the market share, followed by ecstasy and stimulants.
For tier 3, weed is the top product and greatly surpasses the second and third one (pills and cocaine), which have a similar market share.

Market share of tier 4 is not so relevant because there are very few items having this classification compared to the total number of listings:
```{r}
categories %>%
  filter(!is.na(Tier4)) %>%
  count()
```
We may hypotize this happened so due to a wrong classification, either by the user selling the product or by the website itself. However, for our analysis' sake, we may eliminate this less-relevant tier 4 and reassign those items to the upper-level tier. Note that we are not elimining any entry, we are *reclassifying* it.
```{r}
agora <-
  agora %>%
  mutate(Category, Category = ifelse(str_detect(Category, "Sex"),"Info/eBooks/Relationships", Category)) %>%
  mutate(Category, Category = ifelse(str_detect(Category, "trim"),"Drugs/Cannabis/Shake", Category)) %>%
  mutate(Category, Category = ifelse(str_detect(Category, "UFOs"),"Info/eBooks/Aliens", Category))
```
Once we drawn a preliminary profile of the market categories, we now focus on prices.
As in every darkweb market, prices are in expressed in bitcoins. Moreover, the feature is a string which encodes both the price and the currency. We need to make it tidy by converting it and getting rid of the currency, which is always the same.
```{r}
agora <-
  agora %>%
  separate(Price, into = c("PriceBTC", "Currency"), sep = " ") %>%
  mutate(PriceBTC = as.numeric(PriceBTC)) %>%
  select(-c("Currency"))
agora
```
We validate the new feature by checking for null values.
```{r}
agora %>%
  filter(is.na(PriceBTC))
```
We found some other misparsed entries. Luckily they are a few, so we drop them.
```{r}
agora <-
  agora %>%
  filter(!is.na(PriceBTC))
```
Despite the value being now tidy, we could improve its representation. Prices expressed in Bitcoin are less informative than prices expressed in standard currencies. This mainly for two reasons:

* Agora dataset is from years 2014/2015. Bitcoin value back in those years is probably very different from its current one.

* People are used to express values with standard currencies. Knowing that a price is 0.025 BTC give a less precise idea than knowing it is 3882 euro, even for those who does not use it as their main currency.

So we need to convert the original price to another currency. The problem is that Bitcoin is very volatile: its value quickly changes on a daily basis, and *we are dealing with data covering a timespan of two years and without an indication about the exact date at which a particular item was on sale*. Thus, we need a method to achieve a meaningful enough conversion factor to craft a new price feature.
For this task, we use a dataset which we extracted from [Investing.com](https://www.investing.com/), a reputable website offering stock market services. Since Bitcoin/Euro exchange rate is listed on the stock exchange, we can easily find data related to its value over a certain period.
```{r message = FALSE}
btc_eur <- read_csv("data/BTCHist01012014.csv")
btc_eur
```
This dataset is a snapshot of the financial market related to BTC/EUR exchange rate over the time period 01/01/2014 - 30/11/2020.
A market analysis on the fluctuation of Bitcoin value is complex and out of the scope of this project, but to have an idea of the volatility of this cryptocurrency we can see the first and last entry of this dataset:
```{r}
head(btc_eur, n = 1)
tail(btc_eur, n = 1)
```
We see that on date 01/01/2014 the BTC/EUR exchange rate was `815.9`. Almost six years later, on 30/11/2020, the same rate is `18646.0`.
But we don't need to look over a very wide timespan to understand Bitcoin's volatility. Let's compare years of the period which matters to us, namely years 2014-2015:
```{r}
btc_eur %>%
  filter(Date == "Jan 01, 2014") %>%
  head(n = 1)
```
On 01/01/2014, one bitcoin was worth `815.9` euros.
```{r}
btc_eur %>%
  filter(Date == "Jan 01, 2015") %>%
  head(n = 1)
```
Exactly one year later, its value is less than a half!
This clearly may represent a problem in our analysis, because we lack a time reference indicating the exact item date of sale. So we use this dataset as a mean to obtain an exchange rate which is as reliable as possible.
First we clean and filter the dataset to suit our needs. We are only interested to the date and the price in a specific timespan: from a [quick search](https://en.wikipedia.org/wiki/Agora_%28online_marketplace%29) we see that the website shut down in August 2015, so we can narrow the interval.
```{r}
btc_eur <-
  btc_eur %>%
  select(`Date`, Price) %>%
  mutate(`Date` = mdy(`Date`)) %>%
  filter(`Date` < as_date("2015-08-31"))
btc_eur
```
```{r}
btc_eur %>%
  ggplot(aes(x = Date, y = Price)) +
  geom_line(na.rm = TRUE) +
  labs(x = "Date", y = "BTC/EUR") +
  scale_y_continuous(labels = scales::dollar_format(suffix = "€", prefix = ""))
```
As we expected, exchange rate was quite variable in these years too. We compute the average value and take it as conversion factor:
```{r}
btc_eur_rate <-
  mean(btc_eur[["Price"]])
btc_eur_rate
```
We can now create a new column in our dataset.
```{r}
agora <-
  agora %>%
  mutate(PriceEUR = round(PriceBTC * btc_eur_rate, 2), .after = PriceBTC) %>%
  mutate(PriceBTC = round(PriceBTC, 4))
agora
```



Now that we have reliable pricing info, we can go back on the categories information to obtain new insights (remember that cat. tier 4 has been merged with tier 3).
```{r}
categories <-
  agora %>%
  mutate(Depth = str_count(Category, "/") + 1) %>%
  mutate(Depth = as_factor(Depth)) %>%
  separate(Category, into = c("Tier1", "Tier2", "Tier3"), sep = "/")
```
We use a boxplot to visualize the prices per category tier.
```{r}
categories %>%
  ggplot(aes(x = Depth, y = PriceEUR)) + 
  geom_boxplot(outlier.colour = "red", outlier.shape = 1, outlier.size = 4) +
  scale_y_log10(labels = scales::dollar_format(suffix = "€", prefix = ""), 
                breaks = scales::trans_breaks("log10", function(x) 10^x)) +
  labs(title = "Prices per Category Tier", x = "Category Tier", y = "Price")
```
We see a noticeable number of red circles. However, this does not automatically classify them as outliers: identification of outliers is tricky and should not be done only relying on boxplots only. This is even more true when dealing with market prices, where many market-related parameters should be taken into account. More probably, these outliers are just values that do not fall in a normal distribution: in other words, there is a number of items whose price is very high and far away from the median price and the 75th percentile.

Zooming on the other categories, we notice the same behavior:
```{r}
categories %>%
  select(Tier1, PriceEUR) %>%
  drop_na() %>%
  ggplot(aes(x = Tier1, y = PriceEUR)) + 
  geom_boxplot(outlier.colour = "red", outlier.shape = 1, outlier.size = 4) +
  scale_y_log10(labels = scales::dollar_format(suffix = "€", prefix = ""),
                breaks = scales::trans_breaks("log10", function(x) 10^x)) +
  scale_x_discrete(guide = guide_axis(n.dodge=3)) +
  labs(title = "Detailed Prices for Category Tier 1", x = "Category", y = "Price")

categories %>%
  select(Tier2, PriceEUR) %>%
  drop_na() %>%
  ggplot(aes(Tier2, PriceEUR)) + 
  geom_boxplot(outlier.colour = "red", outlier.shape = 1, outlier.size = 4) +
  scale_y_log10(labels = scales::dollar_format(suffix = "€", prefix = ""),
                breaks = scales::trans_breaks("log10", function(x) 10^x)) +
  scale_x_discrete(guide = guide_axis(n.dodge = 7)) +
  labs(title = "Detailed Prices for Category Tier 2", x = "Category", y = "Price")

categories %>%
  select(Tier3, PriceEUR) %>%
  drop_na() %>%
  ggplot(aes(Tier3, PriceEUR)) + 
  geom_boxplot(outlier.colour = "red", outlier.shape = 1, outlier.size = 4) +
  scale_y_log10(labels = scales::dollar_format(suffix = "€", prefix = ""),
                breaks = scales::trans_breaks("log10", function(x) 10^x)) +
  scale_x_discrete(guide = guide_axis(n.dodge = 7)) +
  labs(title = "Detailed Prices for Category Tier 3", x = "Category", y = "Price")
```
However, these boxplot give a nice visual feedback about the price distribution on different levels. For example, in each of them we can see that prices for some categories, such as Drugs, fluctuates a lot. On the other hand, it classifies the `Price` as quite unreliable: even through the process we followed to convert it from BTC to Euros, we still cannot be sure we obtained a precise value due to the intrinsic instability of the cryptocurrency and the lack of a precise time reference.

###Shipping information and routes
We focus on the shipping information, which is represented from the `Origin` and `Destination` columns.
```{r}
destination <-
  agora %>%
  select(Destination) %>%
  mutate(Destination = str_to_upper(Destination)) %>%
  arrange(Destination) %>%
  distinct()

origin <-
  agora %>%
  select(Origin) %>%
  mutate(Origin = str_to_upper(Origin)) %>%
  arrange(Origin) %>%
  distinct()
```
We immediately notice some big problems. *No coherent naming system was used to represent countries*. Sometimes the feature is a country code, sometimes it is an alias, sometimes it is a list of countries or a value that does not represent a country at all. By wiping out all the invalid entries we would probably lose too much data, so we need to clean it in some way.

`Origin` contains 360 distinct values, while `Destination` contains 971 of them. Despite the number, in this case the most viable method to both keep data and make it reliable is to clean them manually: an effective automatic processing which also retains an accettable number of entries is not easy to apply in this case. So the approach we will use is the following: first, we export the two columns in a .csv file each.
```{r}
destination %>%
  write_csv("data/destination_raw.csv")

origin %>%
  write_csv("data/origin_raw.csv")
```
Then *we edit the file with a text editor*, and add a new `CountryCode` which we manually edit by adopting the following rules:

* Each country string is mapped to its ISO alpha-3 country code.

* Continents are mapped to a 2-digit code (EUROPE -> EU).

* Worldwide shipping is represented by the code `WW`.

* In case where more than a country/continent is listed, their corresponding ISO codes will be concatenated with a comma as separator (Europe, Usa, Asia -> EU,USA,AS)

* In case a country/continent is excluded, a ! is prepended to its country code (WORLDWIDE EXCEPT AUSTRALIA -> WW,!AUS)

* When possible, a country list is summarized by the most general one. For example, EUROPE AND GERMANY -> EU since Germany is in Europe.

* Continent classification respects political boundaries of years 2014-2015. For example, Great Britain was still a member of European Union.

* Unknown countries and those who cannot be described by means of the aforementioned rules are left empty or ignored: this happens especially for geographical area which cannot be expressed in terms of proper countries or continents, such as "South America" or "Scandinavia".

* As explained on the Kaggle dataset description, a missing `Destination` or `Origin` value probably means that the item ships worldwide. This is also mentioned on other website that hosted the dataset, such as [this one](https://archive.org/details/dnmarchives). Given this, we assume this exact meaning, and replace `NA` with `WW` for both `Origin` and `Destination`.

Once we edited the files, we re-import them.
```{r message = FALSE}
destination_clean <- read_csv2("data/destination_clean.csv")
origin_clean <- read_csv2("data/origin_clean.csv")
```
We then join the two cleaned datasets with the original dataset and replace the `Origin` and `Destination` column with their clean versions, and drop rows which could not be cleaned (recall we exported them in uppercase to discard some duplicates, so we need to do this in the original dataset too).
```{r}
agora <-
  agora %>%
  mutate(Destination = str_to_upper(Destination)) %>%
  mutate(Origin = str_to_upper(Origin)) %>%
  inner_join(origin_clean) %>%
  inner_join(destination_clean) %>%
  select(-c(Origin, Destination)) %>%
  rename(Origin = ShipFrom, Destination = ShipTo) %>%
  drop_na(Origin, Destination)
```
We now have `Origin` and `Destination` expressed with a coherent notation. The dataset is not tidy yet, though: several entries have more than a country listed as origin or destination.
```{r}
agora %>%
  filter(str_length(Destination) > 3)
agora %>%
  filter(str_length(Origin) > 3)
```
We could try solving this problem by inserting a duplicated row for each country listed in `Origin` and `Destination`. For example, from item `X` shipping to `USA,CAN` we obtain two rows: `X` shipping to `USA` and `X` shipping to `CAN`. This approach arises raises a new problem: several country lists contain countries where the item does not ship from/to, and splitting them would create meaningless entries. For example, separating an entry for item `Y` which ships to `WW,!AUS` would create two rows: item `Y` shipping to `WW` and item `Y` shipping to `!AUS`. This is clearly wrong, since we obtain incorrect informations. So, prior separating rows, we discard the ones containing country exclusions in shipping lists. We will lose some data (~ 10000 rows), but on the other hand our analysis will be more correct.
```{r}
agora_no_excl <-
  agora %>%
  filter(!str_detect(Origin, "!")) %>%
  filter(!str_detect(Destination, "!"))
```
We can now duplicate rows depending on the countries in the two columns
```{r}
agora_no_excl <-
  agora_no_excl %>%
  separate_rows(Origin, sep = ",") %>%
  separate_rows(Destination, sep = ",") %>%
  distinct()
```
We managed to clean the `Origin` and `Destination` column and may now start with shipping-related exploration.
As a first insight, we want to know the major sources and destinations for the market products.
```{r}
top_dest <-
  agora_no_excl %>%
  group_by(Origin) %>%
  summarise(N_items = n()) %>%
  arrange(desc(N_items)) %>%
  slice_head(n = 20) %>%
  ggplot(aes(x = N_items, y = reorder(Origin, N_items))) +
  geom_col() +
  labs(title = 'Top 30 origin countries', x = 'No. of items on sale', y = 'Country')

top_sources <-
  agora_no_excl %>%
  group_by(Destination) %>%
  summarise(N_items = n()) %>%
  arrange(desc(N_items)) %>%
  slice_head(n = 20) %>%
  ggplot(aes(x = N_items, y = reorder(Destination, N_items))) +
  geom_col() +
  labs(title = 'Top 30 destination countries', x = 'No. of items on sale', y = 'Country')  
  
grid.arrange(top_dest, top_sources, ncol = 2)
```
With respect to items origin, USA dominates the market and also for shipping destinations it places itself in the top positions. In both plots we also find Europe, Australia, Germany and Canada as main origins/destinations, while the other countries have a negligible share. However, most vendors declare they are able to ship their products worldwide.

We now look for the most used market routes:
```{r}
sales_channel <-
  agora_no_excl %>%
  group_by(Origin, Destination) %>%
  summarise(`Sales Volume %` = round(n() / nrow(agora_no_excl) * 100, digits = 4)) %>%
  arrange(desc(`Sales Volume %`))
sales_channel
```
Again, on a total of 349 routes the American one stands out as the preferred, with roughly a 15% of the sales taking place internally to the USA. Exluding worldwide shipments, the second and third countries in the list are United Kingdom and Germany, which together convey a cumulative sales volume of 10% ca.
Moreover, if we scroll through the dataset entries, we note that most of them have a very low sales volume, meaning that few items are shipped to or ship from those countries.
However, more than a half of the market sales ships worldwide so even countries with low-volume or non-existent shipping routes can receive most of the items.
```{r}
sales_channel %>%
  ungroup() %>%
  filter(Destination == "WW") %>%
  summarize(`Items shipped worldwide %` = sum(`Sales Volume %`))
```

To focus on the most important shipping routes, we define a threshold for the sales volume to filter out entries with a negligible traffic. Moreover, we exclude worldwide shipping to focus on the single countries.
```{r}
SalesVolumeThreshold = 0.1
top_sales_channel <-
  sales_channel %>%
  filter(`Sales Volume %` >= SalesVolumeThreshold) %>%
  filter(Origin != "WW") %>%
  filter(Destination != "WW")
```
The following plot shows a tile representation of the top shipping routes with respect their importance in terms of sales volume.
```{r}
top_sales_channel %>%
  ggplot(aes(x = Origin, y = Destination)) +
  geom_tile(aes(fill = `Sales Volume %`)) +
  scale_x_discrete(guide = guide_axis(n.dodge = 2))
```
*Additional: investigate countries where items DO NOT ship?*
By analyzing the shipping information, we noted that often some countries are explicitly excluded from shipping destinations. We concentrate our exploration on such countries, by doing exactly the same procedure we just carried on to explore shipping routes. The only difference is that we only retain countries where items are declared to *not* ship to.
```{r}
agora_excl <-
  agora %>%
  separate_rows(Destination, sep = ",") %>%
  filter(str_detect(Destination, "!"))
agora_excl
```
Now, for each country, we calculate the percentage of items that *does not ship* to it (percentage is calculated on the total number of listings).
```{r}
sales_channel_excl <-
  agora_excl %>%
  mutate(Destination = str_sub(Destination, 2)) %>%
  group_by(Destination) %>%
  summarise(`Shipping Exclusion %` = round(n() / nrow(agora) * 100, digits = 4)) %>%
  arrange(desc(`Shipping Exclusion %`))
sales_channel_excl
```
24 countries that happen to be excluded as shipping destination. The first of such is again USA, immediately followed by Australia. The interesting fact is that Australia, which has a sales volume which is much less than USA, has a percentage of exclusion which is almost as high: this means that a certain number of items does not ship to Australia.
```{r}
sales_channel_excl %>%
  ggplot(aes(x = reorder(Destination, -`Shipping Exclusion %`), y = `Shipping Exclusion %`)) +
  geom_col() +
  scale_x_discrete(guide = guide_axis(n.dodge = 2)) +
  labs(title = 'Countries per Shipping Exclusion %', x = 'Country', y = '% items not shipping')
```

#Conclusions
Despite the unreliable nature of the dataset, we managed to analyze various facets of the Agora market and to obtain interesting insights.
A large number of vendors was hosted in the market, but a handful of individuals dominates it in terms of number of items sold. Due to the type of goods offered, it was important
for a buyer to have at least a basic guarantee regarding the purchases, since it might be difficult to track the shipment or to get a refund. For this reason a vendor's reputation,
expressed by their rating, presumably played an important role: it indicated that a certain number of transactions was successfully closed with a certain satisfaction degree.
High ratings were common among market's vendors, meaning that Agora seemed to offer a reliable service. In fact, it was known to be one of the biggest platforms of its category.
A wide range of items from many different categories was available on the market: however, categorization was rough and not so defined nor reliable. Despite this, we found out that
drugs and related products were by far the most popular category, especially cannabis and derivates.
Pricing analysis is not precise due to several factors: the use of Bitcoin as currency, which is very unstable, and the lack of a precise time reference for each entry. The resulting
currency conversion gives an idea of the item's price in a standard currency, but it has to be taken with a grain of salt. Price seems to fluctuate a lot between items of the same category
or subcategory, and there are some items with incredibly high prices that are quite suspicious in terms of authenticity.
Shipping-related information, as it was provided, was useless and not suitable for exploration or analysis.
A lot of preprocessing, including manual manipulation and the adoption of a custom notation for representing countries and continents, was required in order to clean it.
However, we were able to get valuable insights by identifiying top countries and the most used shipping routes of the market. Those information could be useful to
identify high risk zones for what concerns traffic of illegal items. Many items were declared to ship worldwide, but individually USA was by far the country in which most of the transaction 
took place. Australia, United Kingdom, Canada, Germany and the European Union in general were the other "hot spots" of those years both for outgoing and ingoing traffic.
We also analyzed countries that happend to be frequently excluded from shipments: Australia turned out to be one of the destinations to which items were often not shipped. This
might be justified with the fact that Australia was known, at the time but also nowadays, for its strict laws concerning importation of goods.

To stress the concept: this dataset represents just an excerpt on a given time window of a market that, in its whole lifecycle, hosted an uncountable number of items and transactions.
The given dataset surely is insufficient to give a full characterization of the whole market platform and its behaviors, even without considering its incompleteness and limits.
However, this does not devalues our analysis: it only means that it is important that the results are treated by taking into account what we just pointed out.


